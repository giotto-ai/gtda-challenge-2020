{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules before executing user code\n",
    "%load_ext autoreload\n",
    "# reload all modules every time before executing the Python code\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:16:56.824460Z",
     "start_time": "2020-11-15T22:16:56.701354Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install gensim\n",
    "!{sys.executable} -m pip install numpy scipy\n",
    "!{sys.executable} -m pip install tsfresh matplotlib glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T23:26:45.523400Z",
     "start_time": "2020-11-15T23:26:45.498090Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of disease type from topological features of time series\n",
    "(Submission: G. Petri, A. Leitao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Diseases and their spreading patterns have been studied object of attention for a long time, both from the biological and epidemiological perspective. \n",
    "\n",
    "Being able to characterize both aspects is crucial for a timely response to outbreaks at the individual and population level.  This is of even greater importance for emerging epidemics, of which COVID-19 is just the most tragic and recent example. \n",
    "\n",
    "Unfortunately, it can take a long time before researchers are able to pinpoint the spreading mechanisms for a specific novel pathogen. However, what if we could use the epidemic curve of a novel pathogen to predict what is the known disease that it resembles the most? \n",
    "In this case, we would be able to leverage interventions that have been developed for the latter disease to try and mitigate the former. \n",
    "\n",
    "In order to this, we first need to learn whether we can learn what known diseases look like in terms of their epidemic curves. The aim of this notebook is to show that --employing topological features obtained from time series-- it is possible to accurately predict the disease from a set of epidemic curves. \n",
    "\n",
    "In particular, we build on recent work on the [structure of predictability of epidemic outbreaks](https://www.nature.com/articles/s41467-019-08616-0) and construct embeddings that use both the temporal and ordinal properties of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "The data (included with the notebook) is a selection (N=454) of the US epidemic timeseries used in [Scarpino&Petri](https://www.nature.com/articles/s41467-019-08616-0), spanning in some cases more than 20 years. \n",
    "The time resolution is weekly. \n",
    "We note that the timeseries are of different lengths, start and end at different instants in time, and at times contain intervals of missing values. \n",
    "We deliberately leave also the noisy and faulty timeseries within the dataset in order to highlight the (honestly surprising) robustness of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:16:59.235993Z",
     "start_time": "2020-11-15T22:16:58.119320Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "fs = glob.glob('./limits/*.csv');\n",
    "\n",
    "X_ts, y_labels = [], []\n",
    "for f in fs[:460]:\n",
    "    data = pd.read_csv(f);\n",
    "    X_ts.append(data.fillna(0).x);\n",
    "    y_labels.append(f.split('/')[-1].split('-')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show below a random selection of epidemic curves from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:17:00.164773Z",
     "start_time": "2020-11-15T22:16:59.238227Z"
    }
   },
   "outputs": [],
   "source": [
    "iss = np.random.randint(0,455, 8)\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "count=1\n",
    "for i in iss:\n",
    "    plt.subplot(2,4, count)\n",
    "    X_ts[i].plot()\n",
    "    plt.title(y_labels[i])\n",
    "    count+=1\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T21:59:23.040499Z",
     "start_time": "2020-11-15T21:59:23.036525Z"
    }
   },
   "source": [
    "The selection of timeseries also provides a rather balanced class assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:17:00.169648Z",
     "start_time": "2020-11-15T22:17:00.166898Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "The analysis is as follows:\n",
    "1. we import the timeseries and transform them into their symbolic version (a la permutation entropy)\n",
    "2. from the symbolized timeseries, we create embeddings using Word2Vec;\n",
    "3. we then compute VR persistent homology on the resulting embeddings and extract simple features from the persistence diagrams (amplitude, persistence entropy, Betti numbers);\n",
    "4. We then feed these features to a classifier to learn disease types.\n",
    "5. In parallel, we also repeat the same analysis on the Takens embeddings extracted from the epidemic timeseries.\n",
    "6. Finally, we join the two sets of features, coming from symbolic and Takens embeddings, and show that their combination is able to outperform a classifier based on standard timeseries features. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Epidemic timeseries and their symbolization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:23:41.914226Z",
     "start_time": "2020-11-15T22:17:00.173392Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import randn, randint\n",
    "\n",
    "def noisy_symbolic_sequence(time_series, m, delay,noise_amp=0.3):\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    n = len(time_series)\n",
    "    permutations = np.array(list(itertools.permutations(range(m))))\n",
    "    c = [0] * len(permutations)\n",
    "    new_sequence = []\n",
    "    for i in range(n - delay * (m - 1)):\n",
    "        sorted_index_array = ''.join(map(str,np.argsort(time_series[i:i + delay * m:delay] + noise_amp*randn(len(time_series[i:i + delay * m:delay])),\n",
    "                                                 kind='quicksort')))\n",
    "        new_sequence.append(sorted_index_array);\n",
    "    return new_sequence;\n",
    "\n",
    "text = []\n",
    "for x in X_ts:\n",
    "    text.append(noisy_symbolic_sequence(x, 5, 1, noise_amp=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T21:22:47.522657Z",
     "start_time": "2020-11-15T21:22:46.685756Z"
    }
   },
   "source": [
    "We associate to each timeseries a symbolic timeseries built from the ordinal patterns in short strings of values. \n",
    "This is the same procedure that is performed before computing permutation entropy, details can be found in the paper linked in the introduction. \n",
    "\n",
    "The function above to extract the symbolic timeseries includes a source of noise, which is used to remove draws in the construction of the ordinal patterns. It also simulates the noise that is likely present in disease incidence measurements. Similarly to Scarpino&Petri, we set the delay time to 1 to have temporally continuous windows, and  the embedding dimension to 5 (which is the typical value observed in previous studies for weekly timeseries).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of Word2vec embeddings for symbolized timeseries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:23:57.219103Z",
     "start_time": "2020-11-15T22:23:41.918839Z"
    }
   },
   "outputs": [],
   "source": [
    "# node2vec parameters\n",
    "param_p = 1.0\n",
    "param_q = 1.0\n",
    "param_dimensions = 10\n",
    "param_window = 4\n",
    "param_num_workers = 8\n",
    "param_iter = 1\n",
    "\n",
    "# compute \"word\" embeddings\n",
    "Xs = []\n",
    "for tt in text:\n",
    "    model = Word2Vec(sentences=[tt],\n",
    "                 size=param_dimensions,\n",
    "                 window=param_window,\n",
    "                 min_count=0,\n",
    "                 sg=1,\n",
    "                 negative=5,\n",
    "                 iter=param_iter,\n",
    "                 workers=param_num_workers);\n",
    "    Xs.append(model.wv.vectors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of VR persistence on symbolic embeddings\n",
    "For illustration purposes we separate the computation of homology and diagrams for the two studied dimensions [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:23:57.224256Z",
     "start_time": "2020-11-15T22:23:57.221014Z"
    }
   },
   "outputs": [],
   "source": [
    "symbolic_diags = {}\n",
    "amps_dict, ents_dict, betti_dict = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:25:05.335274Z",
     "start_time": "2020-11-15T22:23:57.226365Z"
    }
   },
   "outputs": [],
   "source": [
    "from gtda.diagrams import Amplitude, PersistenceEntropy, NumberOfPoints\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "dim = 0\n",
    "VR = VietorisRipsPersistence(metric='correlation', homology_dimensions=[dim])\n",
    "VR.fit(Xs)\n",
    "symbolic_diags[dim] = VR.transform(Xs)\n",
    "amps_dict[dim] = Amplitude(metric='heat', metric_params={'sigma': 0.05}).fit_transform(symbolic_diags[dim])\n",
    "ents_dict[dim] = PersistenceEntropy(normalize=True).fit_transform(symbolic_diags[dim])\n",
    "betti_dict[dim] = NumberOfPoints().fit_transform(symbolic_diags[dim])\n",
    "\n",
    "dim = 1\n",
    "VR = VietorisRipsPersistence(metric='correlation', homology_dimensions=[dim])\n",
    "VR.fit(Xs)\n",
    "symbolic_diags[dim] = VR.transform(Xs)\n",
    "amps_dict[dim] = Amplitude(metric='heat', metric_params={'sigma': 0.05}).fit_transform(symbolic_diags[dim])\n",
    "ents_dict[dim] = PersistenceEntropy(normalize=True).fit_transform(symbolic_diags[dim])\n",
    "betti_dict[dim] = NumberOfPoints().fit_transform(symbolic_diags[dim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining together topological features of different dimensions from symbolic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:37:30.119213Z",
     "start_time": "2020-11-15T22:37:30.114037Z"
    }
   },
   "outputs": [],
   "source": [
    "X_features = np.array(list(zip([x[0] for x in amps_dict[0]], [x[0] for x in ents_dict[0]], \n",
    "                               [x[0] for x in amps_dict[1]], [x[0] for x in ents_dict[1]],\n",
    "                               [x[0] for x in betti_dict[1]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction of disease type using only features from symbolic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:37:34.758500Z",
     "start_time": "2020-11-15T22:37:34.637943Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "summary_dict = {}\n",
    "summary_dict['symbolic only'] = []\n",
    "\n",
    "X, y = X_features, y_labels\n",
    "\n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, X, y, cv=5)\n",
    "print('GNB', scores.mean(), scores.std())\n",
    "summary_dict['symbolic only'].append(scores.mean())\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=int(time()))\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print('Random Forest Classifier', scores.mean(), scores.std())\n",
    "summary_dict['symbolic only'].append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of illustration, we show here and in the following results for two different classifiers, one baised on Naive Bayes and the other a standard Random Forest classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of Takens embeddings for epidemic timeseries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:29:26.034272Z",
     "start_time": "2020-11-15T22:25:05.573735Z"
    }
   },
   "outputs": [],
   "source": [
    "import gtda.time_series as ts\n",
    "\n",
    "te_X, te_diags = [], []\n",
    "te_diags = {}\n",
    "te_amps_dict, te_ents_dict, te_betti_dict = {}, {}, {}\n",
    "for i, x in enumerate(X_ts): \n",
    "    te_diags[i] = {}\n",
    "    te_amps_dict[i], te_ents_dict[i], te_betti_dict[i] ={}, {}, {}\n",
    "    te_X.append(ts.SingleTakensEmbedding(parameters_type='fixed', time_delay=1, dimension=5, stride=5).fit_transform(x))\n",
    "    for dim in [0,1]:\n",
    "        teVR = VietorisRipsPersistence(metric='euclidean', homology_dimensions=[dim])\n",
    "        teVR.fit([te_X[-1]])\n",
    "        te_diags[i][dim] = teVR.transform([te_X[-1]]);\n",
    "        te_amps_dict[i][dim] = Amplitude(metric='heat', metric_params={'sigma': 0.05}).fit_transform(te_diags[i][dim])\n",
    "        te_ents_dict[i][dim] = PersistenceEntropy(normalize=True).fit_transform(te_diags[i][dim])\n",
    "        te_betti_dict[i][dim] = NumberOfPoints().fit_transform(te_diags[i][dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reformat the features dictionaries to be amenable to Giotto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:29:26.041428Z",
     "start_time": "2020-11-15T22:29:26.036401Z"
    }
   },
   "outputs": [],
   "source": [
    "te_features = []\n",
    "for i, x in enumerate(X_ts):\n",
    "    feat = []\n",
    "    for dim in [0,1]:\n",
    "        feat.extend([te_amps_dict[i][dim][0][0], te_ents_dict[i][dim][0][0], te_betti_dict[i][dim][0][0]])\n",
    "    te_features.append(feat)\n",
    "\n",
    "teX = np.array(te_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:07:58.290449Z",
     "start_time": "2020-11-15T22:07:58.288527Z"
    }
   },
   "source": [
    "### prediction of disease type using only features from Takens embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:29:26.166058Z",
     "start_time": "2020-11-15T22:29:26.042930Z"
    }
   },
   "outputs": [],
   "source": [
    "X = teX\n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, X, y, cv=5)\n",
    "print('GNB', scores.mean(), scores.std())\n",
    "summary_dict['Takens only'] = []\n",
    "summary_dict['Takens only'].append(scores.mean())\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=int(time()))\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print('Random Forest Classifier', scores.mean(), scores.std())\n",
    "\n",
    "summary_dict['Takens only'].append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction of disease type using  features from symbolic and Takens embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:29:26.170587Z",
     "start_time": "2020-11-15T22:29:26.167945Z"
    }
   },
   "outputs": [],
   "source": [
    "joinX = np.concatenate([X_features, teX], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:29:26.283024Z",
     "start_time": "2020-11-15T22:29:26.172480Z"
    }
   },
   "outputs": [],
   "source": [
    "X = joinX\n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, X, y, cv=5)\n",
    "print('GNB', scores.mean(), scores.std())\n",
    "summary_dict['Symbolic + Takens'] = []\n",
    "summary_dict['Symbolic + Takens'].append(scores.mean())\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=int(time()))\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print('Random Forest Classifier', scores.mean(), scores.std())\n",
    "summary_dict['Symbolic + Takens'].append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with standard timeseries features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract here a set of standard timeseries features. \n",
    "In particular, we choose standard ones that can be compared across timeseries of different lenght and composition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:29:26.764648Z",
     "start_time": "2020-11-15T22:29:26.284884Z"
    }
   },
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction.feature_calculators import abs_energy, mean,  skewness, kurtosis, median\n",
    "from tsfresh.feature_extraction.feature_calculators import sample_entropy, variance, mean_abs_change, mean_second_derivative_central \n",
    "\n",
    "func_list = [abs_energy, mean, skewness, sample_entropy, variance, mean_abs_change, mean_second_derivative_central, kurtosis, median]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:32:57.828005Z",
     "start_time": "2020-11-15T22:29:26.766361Z"
    }
   },
   "outputs": [],
   "source": [
    "features_X_ts = []\n",
    "\n",
    "for i, x in enumerate(X_ts):\n",
    "    feat = [func(x) for func in func_list]\n",
    "    features_X_ts.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:32:57.952807Z",
     "start_time": "2020-11-15T22:32:57.829888Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(features_X_ts)\n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, X, y, cv=5)\n",
    "print('GNB', scores.mean(), scores.std())\n",
    "summary_dict['Standard feats'] = []\n",
    "summary_dict['Standard feats'].append(scores.mean())\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=int(time()))\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print('Random Forest Classifier', scores.mean(), scores.std())\n",
    "summary_dict['Standard feats'].append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:09:53.456616Z",
     "start_time": "2020-11-15T22:09:53.454327Z"
    }
   },
   "source": [
    "### prediction of disease type using all features (symbolic, Takens, standards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:32:58.064660Z",
     "start_time": "2020-11-15T22:32:57.954654Z"
    }
   },
   "outputs": [],
   "source": [
    "megajoinX = np.concatenate([X_features, teX, np.array(features_X_ts)], axis=1)\n",
    "\n",
    "X = megajoinX \n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, X, y, cv=5)\n",
    "print('GNB', scores.mean(), scores.std())\n",
    "summary_dict['All feats'] = []\n",
    "summary_dict['All feats'].append(scores.mean())\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=int(time()))\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean(), scores.std())\n",
    "summary_dict['All feats'].append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:11:29.857228Z",
     "start_time": "2020-11-15T22:11:29.854738Z"
    }
   },
   "source": [
    "## Summary of results\n",
    "We find that both classifiers already achieve a large accuracy using only symbolic features (with respect to the random baseline \\~12%), and naturally the union with the Takens ones allowed both classifiers to increase the accuracy. \n",
    "We find that symbolic features alone are sufficient to outperform the standard ones, while using all the features together allows to reach very high (\\~87%) accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:32:58.086710Z",
     "start_time": "2020-11-15T22:32:58.074794Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(summary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it is possible to analyse which disease is better described, and we find that the airbone pathogens (and Chlamydia)\n",
    "seem to be the most identifiable ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:32:58.111723Z",
     "start_time": "2020-11-15T22:32:58.088683Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "probs = pd.DataFrame(clf.predict_proba(X), columns = clf.classes_)\n",
    "probs['real_label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:32:58.886424Z",
     "start_time": "2020-11-15T22:32:58.113624Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "count = 1\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "for disease, group in probs.groupby('real_label'):\n",
    "    plt.subplot(2,4, count)\n",
    "    xbase = range(len(clf.classes_))\n",
    "    plt.errorbar(xbase, np.mean(group[clf.classes_]), np.std(group[clf.classes_]));\n",
    "    plt.xticks(xbase, clf.classes_, rotation = 30);\n",
    "    ent = np.round(entropy(np.mean(group[clf.classes_])), 2)\n",
    "    plt.title(disease + ', entropy = '+ str(ent))\n",
    "    count+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And one last thing.. what does Covid-19 look like so far?\n",
    "We reproduced the previous pipeline on data obtained from https://github.com/owid/covid-19-data and tried what our classifier based on symbolic information only thought about the current COVID timeseries in EU. \n",
    "In the interest of time, we preprocessed the timeseries as described above and provide the symbolic embeddings below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:46:07.634628Z",
     "start_time": "2020-11-15T22:46:07.617119Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "covid_Xs = pk.load(open('covid-symbolic-embeddings.pck', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:34:41.325044Z",
     "start_time": "2020-11-15T22:34:41.321567Z"
    }
   },
   "outputs": [],
   "source": [
    "csymbolic_diags = {}\n",
    "camps_dict, cents_dict, cbetti_dict = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:35:30.944353Z",
     "start_time": "2020-11-15T22:35:23.283319Z"
    }
   },
   "outputs": [],
   "source": [
    "from gtda.diagrams import Amplitude, PersistenceEntropy, NumberOfPoints\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "dim = 0\n",
    "VR = VietorisRipsPersistence(metric='correlation', homology_dimensions=[dim])\n",
    "VR.fit(list(covid_Xs.values()))\n",
    "csymbolic_diags[dim] = VR.transform(list(covid_Xs.values()))\n",
    "\n",
    "camps_dict[dim] = Amplitude(metric='heat', metric_params={'sigma': 0.05}).fit_transform(csymbolic_diags[dim])\n",
    "cents_dict[dim] = PersistenceEntropy(normalize=True).fit_transform(csymbolic_diags[dim])\n",
    "cbetti_dict[dim] = NumberOfPoints().fit_transform(csymbolic_diags[dim])\n",
    "\n",
    "dim = 1\n",
    "VR = VietorisRipsPersistence(metric='correlation', homology_dimensions=[dim])\n",
    "VR.fit(list(covid_Xs.values()))\n",
    "csymbolic_diags[dim] = VR.transform(list(covid_Xs.values()))\n",
    "camps_dict[dim] = Amplitude(metric='heat', metric_params={'sigma': 0.05}).fit_transform(csymbolic_diags[dim])\n",
    "cents_dict[dim] = PersistenceEntropy(normalize=True).fit_transform(csymbolic_diags[dim])\n",
    "cbetti_dict[dim] = NumberOfPoints().fit_transform(csymbolic_diags[dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:37:44.698597Z",
     "start_time": "2020-11-15T22:37:44.694412Z"
    }
   },
   "outputs": [],
   "source": [
    "cX_features = np.array(list(zip([x[0] for x in camps_dict[0]], [x[0] for x in cents_dict[0]], \n",
    "                               [x[0] for x in camps_dict[1]], [x[0] for x in cents_dict[1]],\n",
    "                               [x[0] for x in cbetti_dict[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T22:47:25.212314Z",
     "start_time": "2020-11-15T22:47:25.184503Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X_features, y)\n",
    "covid_predict = clf.predict(cX_features)\n",
    "Counter(covid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T23:02:49.274149Z",
     "start_time": "2020-11-15T23:02:49.263368Z"
    }
   },
   "outputs": [],
   "source": [
    "list(zip(list(covid_Xs.keys()), covid_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from our work it seems Covid to behave in terms of epidemic curve features like a mixture of Polio and Influenza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
